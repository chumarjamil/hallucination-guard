<p align="center">
  <img src="https://img.shields.io/badge/python-3.10%2B-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/npm-package-red?style=for-the-badge&logo=npm&logoColor=white" alt="npm">
  <img src="https://img.shields.io/badge/typescript-ready-3178C6?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript">
  <img src="https://img.shields.io/badge/license-MIT-green?style=for-the-badge" alt="License: MIT">
  <img src="https://img.shields.io/badge/status-active-brightgreen?style=for-the-badge" alt="Status: Active">
</p>

<h1 align="center">üõ° Hallucination Guard</h1>

<p align="center">
  <strong>The open-source standard for detecting and preventing hallucinations in AI systems.</strong><br><br>
  CLI ¬∑ SDK ¬∑ API ¬∑ Built for developers ¬∑ Production-ready<br><br>
  <code>pip install hallucination-guard</code> ¬∑ <code>npm install hallucination-guard</code>
</p>

---

## The Problem

LLMs hallucinate. They generate confident, fluent text that is **factually wrong** ‚Äî invented dates, swapped locations, fabricated citations.

In production systems ‚Äî healthcare, legal, finance, education ‚Äî this is not a minor inconvenience. **It's a liability.**

There is no standard, developer-friendly tool to programmatically detect these hallucinations before they reach end users.

## The Solution

**Hallucination Guard** is a complete hallucination detection toolkit:

```
Input Text ‚Üí Claim Extraction ‚Üí Fact Verification ‚Üí Risk Scoring ‚Üí Explanation ‚Üí Structured Output
```

| Feature                | Description                                        |
| ---------------------- | -------------------------------------------------- |
| **Claim Extraction**   | spaCy NLP identifies factual statements            |
| **Fact Verification**  | Wikipedia + sentence-transformer semantic matching |
| **Risk Scoring**       | Weighted formula ‚Üí `0.0 ‚Äì 1.0` risk score          |
| **Explanation Engine** | Human-readable explanations per claim              |
| **Text Highlighting**  | Plain-text `‚ö†[‚Ä¶]‚ö†` + colorized CLI output          |
| **Global CLI**         | `hallucination-guard check "text"`                 |
| **Python SDK**         | `from hallucination_guard import detect`           |
| **REST API**           | `hallucination-guard api --port 8000`              |
| **Integrations**       | LangChain, LlamaIndex, RAG, Streamlit              |
| **Node.js / TS SDK**   | `npm install` ‚Äî use from any JS/TS project         |
| **Cross-platform**     | Works on macOS, Linux, Windows                     |

---

## Quick Start

### Python

```bash
pip install hallucination-guard
python -m spacy download en_core_web_sm
```

### Node.js / TypeScript

```bash
npm install hallucination-guard
# or
yarn add hallucination-guard
# or
pnpm add hallucination-guard
```

> **Note:** The npm package requires the Python engine installed via `pip install hallucination-guard`.

### From Source

```bash
git clone https://github.com/chumarjamil/hallucination-guard.git
cd hallucination-guard
pip install -e .
python -m spacy download en_core_web_sm
```

### Try It ‚Äî One Command

```bash
hallucination-guard check "The Eiffel Tower is located in Berlin."
```

### Try It ‚Äî Python

```python
from hallucination_guard import detect

result = detect("The Eiffel Tower is located in Berlin.")
print(result.hallucinated)     # True
print(result.confidence)       # 0.91
print(result.explanation)      # "Detected 1 unsupported claim(s) ‚Ä¶"
```

### Try It ‚Äî TypeScript / JavaScript

```typescript
import { detect } from 'hallucination-guard';

const result = await detect("The Eiffel Tower is located in Berlin.");
console.log(result.hallucinated);       // true
console.log(result.confidence);         // 0.91
console.log(result.explanation);        // "Detected 1 unsupported claim(s) ‚Ä¶"
```

---

## CLI

Hallucination Guard ships with a global CLI powered by Typer + Rich.

### Commands

```bash
# Check inline text
hallucination-guard check "Paris is the capital of Germany."

# Check a file
hallucination-guard file article.txt

# Batch check (JSON array of texts)
hallucination-guard batch inputs.json

# Start REST API server
hallucination-guard api --port 8000

# JSON output mode
hallucination-guard check "Some text" --json

# Verbose debug logging
hallucination-guard check "Some text" --verbose
```

### Sample Output

```
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üõ°  Hallucination Guard ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ        72% Hallucination Risk  [HIGH]                       ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ confidence 28% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
  Total claims   : 2
  Supported      : 0
  Unsupported    : 2
  Avg similarity : 0.2100

  Detected 2 unsupported claim(s) out of 2. Hallucination risk: 72%.

Highlighted Text
  ‚ö†[The Eiffel Tower is located in Berlin and was built in 1920.]‚ö†

‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ #  ‚îÉ Claim                                    ‚îÉ Confidence ‚îÉ Evidence        ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ 1  ‚îÇ The Eiffel Tower is located in Berlin ‚Ä¶  ‚îÇ     0.2100 ‚îÇ The Eiffel ‚Ä¶    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Explanations
  üî¥ [1] The Eiffel Tower is located in Berlin ‚Ä¶ [high]
      This claim could not be verified ‚Ä¶
```

---

## Python SDK

Three top-level functions for instant integration:

### `detect(text)` ‚Äî Full Pipeline

```python
from hallucination_guard import detect

result = detect("Albert Einstein invented the telephone in 1876.")

result.hallucinated         # True
result.hallucination_risk   # 0.68
result.confidence           # 0.32
result.flagged_claims       # [{"claim": "...", "confidence": 0.12, ...}]
result.explanations         # [Explanation(claim="...", severity="high", ...)]
result.highlighted_text     # "‚ö†[Albert Einstein invented ‚Ä¶]‚ö†"
result.explanation          # "Detected 1 unsupported claim(s) ‚Ä¶"
result.to_dict()            # Full JSON-serializable dict
```

### `score(text)` ‚Äî Risk Score Only

```python
from hallucination_guard import score

risk = score("The Great Wall was built by NASA.")
print(risk)  # 0.72
```

### `explain(text)` ‚Äî Structured Explanation

```python
from hallucination_guard import explain

info = explain("Mars is the largest planet in the solar system.")
# {
#   "hallucinated": True,
#   "confidence": 0.31,
#   "explanation": "Detected 1 unsupported claim(s) ‚Ä¶",
#   "claims": [
#     {
#       "claim": "Mars is the largest planet ‚Ä¶",
#       "hallucinated": True,
#       "confidence": 0.31,
#       "explanation": "This claim could not be verified ‚Ä¶",
#       "severity": "medium",
#       "source": "Wikipedia: Mars"
#     }
#   ]
# }
```

### Advanced: Direct Guard Instance

```python
from hallucination_guard import HallucinationGuard

guard = HallucinationGuard(
    spacy_model="en_core_web_lg",
    transformer_model="all-mpnet-base-v2",
    wiki_lang="en",
)

result = guard.detect("Your text here.")
```

---

## Node.js / TypeScript SDK

Same three functions, async-friendly:

### `detect(text)` ‚Äî Full Pipeline

```typescript
import { detect } from 'hallucination-guard';

const result = await detect("Albert Einstein invented the telephone.");

console.log(result.hallucinated);       // true
console.log(result.hallucination_risk); // 0.68
console.log(result.flagged_claims);     // [{claim: "...", confidence: 0.12}]
console.log(result.highlighted_text);   // "‚ö†[Albert Einstein ‚Ä¶]‚ö†"
```

### `score(text)` ‚Äî Risk Score Only

```typescript
import { score } from 'hallucination-guard';

const risk = await score("The Great Wall was built by NASA.");
console.log(risk); // 0.72
```

### `explain(text)` ‚Äî Structured Explanation

```typescript
import { explain } from 'hallucination-guard';

const info = await explain("Mars is the largest planet.");
console.log(info.hallucinated); // true
console.log(info.claims);       // [{claim: "...", severity: "high"}]
```

### API Mode (No CLI Needed)

If you prefer not to call the CLI subprocess, start the API server and use API mode:

```typescript
import { detect } from 'hallucination-guard';

const result = await detect("Some text", {
  mode: "api",
  apiUrl: "http://localhost:8000",
});
```

### Check Installation

```typescript
import { isInstalled } from 'hallucination-guard';

if (await isInstalled()) {
  console.log("Ready to use!");
} else {
  console.log("Run: pip install hallucination-guard");
}
```

---

## REST API

### Start the Server

```bash
hallucination-guard api --port 8000
# or
uvicorn hallucination_guard.api.server:app --port 8000
```

### `GET /health`

```bash
curl http://localhost:8000/health
```

```json
{"status": "ok", "version": "0.2.0"}
```

### `POST /detect`

```bash
curl -X POST http://localhost:8000/detect \
  -H "Content-Type: application/json" \
  -d '{"text": "The Great Wall of China was built in 1995 by NASA."}'
```

```json
{
  "hallucinated": true,
  "hallucination_risk": 0.72,
  "confidence": 0.28,
  "total_claims": 1,
  "supported_claims": 0,
  "unsupported_claims": 1,
  "average_similarity": 0.18,
  "flagged_claims": [
    {
      "claim": "The Great Wall of China was built in 1995 by NASA.",
      "confidence": 0.18,
      "evidence": "The Great Wall of China is a series of fortifications ‚Ä¶",
      "source": "Wikipedia: Great"
    }
  ],
  "explanations": [
    {
      "claim": "The Great Wall of China was built in 1995 by NASA.",
      "hallucinated": true,
      "confidence": 0.18,
      "explanation": "This claim could not be verified ‚Ä¶",
      "severity": "high",
      "source": "Wikipedia: Great"
    }
  ],
  "highlighted_text": "‚ö†[The Great Wall of China was built in 1995 by NASA.]‚ö†",
  "explanation": "Detected 1 unsupported claim(s) out of 1. Hallucination risk: 72%."
}
```

---

## Integrations

Ready-to-use examples for popular AI frameworks in [`examples/`](examples/):

| Integration      | File                        | Description                      |
| ---------------- | --------------------------- | -------------------------------- |
| **LangChain**    | `langchain_integration.py`  | Post-process LLM outputs         |
| **LlamaIndex**   | `llamaindex_integration.py` | Verify RAG responses             |
| **RAG Pipeline** | `rag_pipeline.py`           | Generic `RAGGuard` wrapper class |
| **Streamlit**    | `streamlit_app.py`          | Web UI dashboard                 |
| **REST Client**  | `api_client.py`             | HTTP client example              |
| **SDK**          | `basic_usage.py`            | Python SDK patterns              |

### LangChain Example

```python
from hallucination_guard import detect

def verify_llm_output(response: str, threshold: float = 0.5) -> dict:
    result = detect(response)
    return {
        "response": response,
        "safe_to_use": result.hallucination_risk < threshold,
        "risk": result.hallucination_risk,
        "flagged": result.flagged_claims,
    }
```

### RAG Guard Pattern

```python
from hallucination_guard import detect

class RAGGuard:
    def __init__(self, rag_fn, threshold=0.5):
        self.rag_fn = rag_fn
        self.threshold = threshold

    def query(self, question: str):
        answer = self.rag_fn(question)
        result = detect(answer)
        return {
            "answer": answer,
            "safe": result.hallucination_risk < self.threshold,
            "risk": result.hallucination_risk,
        }
```

---

## Architecture

```
Input Text
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Claim Extractor ‚îÇ  spaCy NLP
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fact Verifier   ‚îÇ  Wikipedia + sentence-transformers
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Risk Scorer     ‚îÇ  Weighted formula ‚Üí 0.0‚Äì1.0
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Explainer       ‚îÇ  Human-readable explanations
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Highlighter     ‚îÇ  ‚ö†[‚Ä¶]‚ö† markers + Rich CLI
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
  DetectionResult
```

### Project Structure

```
.
‚îú‚îÄ‚îÄ src/hallucination_guard/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Public API: detect, score, explain
‚îÇ   ‚îú‚îÄ‚îÄ sdk.py                 # SDK convenience functions
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                 # Typer CLI (check, file, batch, api)
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detector.py        # Pipeline orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claims.py          # Claim extraction (spaCy)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ verifier.py        # Fact verification (Wikipedia + SBERT)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scorer.py          # Risk scoring engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ explainer.py       # Explanation generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ highlight.py       # Text highlighting
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.py          # FastAPI REST server
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ config.py          # Env-based configuration
‚îú‚îÄ‚îÄ npm/                       # Node.js / TypeScript SDK
‚îÇ   ‚îú‚îÄ‚îÄ src/index.ts           # JS/TS client library
‚îÇ   ‚îú‚îÄ‚îÄ bin/cli.js             # npx CLI wrapper
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tests/                     # Unit + integration tests
‚îú‚îÄ‚îÄ examples/                  # Integration examples
‚îú‚îÄ‚îÄ docs/                      # Architecture documentation
‚îú‚îÄ‚îÄ pyproject.toml             # Python package config
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îî‚îÄ‚îÄ LICENSE
```

See [`docs/ARCHITECTURE.md`](docs/ARCHITECTURE.md) for detailed technical documentation.

---

## Configuration

All settings via environment variables or constructor arguments:

| Environment Variable                    | Default            | Description                    |
| --------------------------------------- | ------------------ | ------------------------------ |
| `HALLUCINATION_GUARD_SPACY_MODEL`       | `en_core_web_sm`   | spaCy model                    |
| `HALLUCINATION_GUARD_TRANSFORMER_MODEL` | `all-MiniLM-L6-v2` | Embedding model                |
| `HALLUCINATION_GUARD_WIKI_LANG`         | `en`               | Wikipedia language             |
| `HALLUCINATION_GUARD_SUPPORT_THRESHOLD` | `0.45`             | Min similarity for "supported" |
| `HALLUCINATION_GUARD_HOST`              | `0.0.0.0`          | API bind address               |
| `HALLUCINATION_GUARD_PORT`              | `8000`             | API port                       |
| `HALLUCINATION_GUARD_LOG_LEVEL`         | `INFO`             | Logging level                  |

---

## How It Works

1. **Claim Extraction** ‚Äî spaCy parses the text, identifies factual indicators (*is*, *founded*, *invented*) and named entities, extracts structured claims with SVO triples.

2. **Fact Verification** ‚Äî Each claim queries Wikipedia. Evidence is compared using `all-MiniLM-L6-v2` sentence-transformer, producing cosine-similarity scores.

3. **Risk Scoring** ‚Äî Weighted formula:
   - 50% unsupported claim ratio
   - 35% inverse avg similarity
   - 15% non-linear severity penalty (>50% failure)

4. **Explanation** ‚Äî Each claim gets a human-readable explanation with severity rating (low/medium/high).

5. **Highlighting** ‚Äî Unsupported claims wrapped in `‚ö†[‚Ä¶]‚ö†` markers. CLI renders bold red via Rich.

---

## Development

```bash
git clone https://github.com/chumarjamil/hallucination-guard.git
cd hallucination-guard
make dev          # editable install + dev deps + pre-commit

make test         # run tests with coverage
make lint         # ruff linter
make typecheck    # mypy
make format       # auto-format
make serve        # start API with hot-reload
make demo         # run CLI demo
```

---

## Roadmap

- [x] Claim extraction + fact verification pipeline
- [x] CLI with Rich output
- [x] REST API (FastAPI)
- [x] Python SDK (`detect`, `score`, `explain`)
- [x] Explanation engine with severity ratings
- [x] Node.js / TypeScript SDK (npm package)
- [x] Integration examples (LangChain, LlamaIndex, RAG, Streamlit)
- [ ] LLM-based claim extraction (GPT / Ollama / local)
- [ ] Multi-source verification (Wikidata, PubMed, Knowledge Graph)
- [ ] Async pipeline for concurrent verification
- [ ] Caching layer (Redis / SQLite)
- [ ] Custom knowledge bases
- [ ] Web UI dashboard
- [ ] PyPI package distribution
- [ ] Prometheus metrics export
- [ ] API key authentication
- [ ] Confidence calibration with labeled datasets
- [ ] Webhook notifications for flagged content

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for setup, testing, and PR guidelines.

```bash
make dev && make test
```

---

## License

MIT ‚Äî see [LICENSE](LICENSE).

---

<p align="center">
  <strong>Built for developers who ship AI responsibly.</strong><br>
  <sub>If this helps you, consider giving it a ‚≠ê</sub>
</p>
